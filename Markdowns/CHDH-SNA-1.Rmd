---
title: "CHDH SNA 1"
author: "[Johan Koskinen](https://psychologicalsciences.unimelb.edu.au/research/msps-research-groups/Social_Networks_Laboratory)"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
editor_options: 
  markdown: 
    wrap: 72
bibliography: references.bib
---

```{css, echo=FALSE}
.question {
  background-color: lightpink;
  border: 3px solid red;
  font-weight: bold;
}
```

```{r, include = FALSE}
xfun::download_file("https://raw.githubusercontent.com/johankoskinen/CHDH-SNA/main/Markdowns/references.bib")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

-   Adjacency matrix, basic manipulation
-   Network non-parametric approaches
-   Ego-nets


# Adjacency matrix

## Basic definitions

For a set of nodes $V=\{1,\ldots,n\}$ we define a set of ties as being
un-ordered pairs of nodes, $\{i,j\} \in \{i,j\in V : i\neq j\}$. We can
denote the set of all undordered pairs ${V}\choose{2}$, meaning that the
ties or edges are two-element subsets of $V$, and the set of ties
$E \subseteq {{V}\choose{2}}$.

## Adjacency matrix

For all $\{i,j\} \in {{V}\choose{2}}$, we define the tie-variables
$x_{ij}$ as being indicators $$
\begin{equation*}  
x_{ij}= \left\{
\begin{array}{lr}
    1,&\text{if there is an edge between } i \text{ and } j\\
    0,&\text{else}
\end{array} 
\right. 
\end{equation*}
$$

We collect these in an $n \times n$ **adjacency matrix** $$
X = \begin{bmatrix}
x_{11} & x_{12} & x_{13} & \cdots & x_{1n}\\
x_{21} & x_{22} & x_{23} & \cdots & x_{2n}\\
x_{31} & x_{32} & x_{33} & \cdots & x_{3n}\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & x_{n3} & \cdots & x_{2n}\\
\end{bmatrix}
$$

## Objectives

This is an introduction to social networks using built-in functions in R
and the packages `r 'sna'` and `r 'network'`. We will learn the

-   basic features of the adjacency matrix represented as a `r 'matrix'`
    object,
    -   calculate the degrees of the nodes, and
    -   calculate some fundamental descriptives of the network.
-   We will then translate the network
    -   from a `r 'matrix'` object to a `r 'network'` object in order to
    -   plot the sociogram.

For full details of the packages, see
<https://cran.r-project.org/web/packages/sna/sna.pdf> and
<https://cran.r-project.org/web/packages/network/network.pdf>. For
accessible general R-help see <https://www.statmethods.net/> and for any
kind of errors use <https://google.com>.

This introduction is deliberately writen in **inelegant R**, using as
basic functions as possible. Many packages offer sleeker and more
userfriendly network routines, such as 'igraph'. In particular, I would
like to recomend the packages of David Schooch
<http://mr.schochastics.net/> for accessible and elegant network
analysis in R. In general, basic plots in R (described in
<https://www.statmethods.net/graphs/index.html>) are functional but more
advanced and better looking plots can be acchieved through 'ggplot'.

For basic concepts in network analysis see @robins2015doing and
@borgatti2018analyzing. There is also a handy online bool
<http://faculty.ucr.edu/~hanneman/nettext/> [@hanemanSNA].

## Build your own network

To use `r 'sna'`[@buttsSNA] and `r 'network'` [@buttsNETWORK] for the
first time (*uncomment the install commmands*), install the packages

```{r install, results='hide', warning=FALSE, message=FALSE}
# install.packages("sna")
# install.packages("network")
```

Once packages are installed, load them

```{r loadsna, results='hide', warning=FALSE, message=FALSE}
library("sna")
library("network")
```

### The Matrix

Create an empty **adjacency** matrix for `r 'n = 5'` nodes

```{r adjmat, results='hide'}
n <- 5
ADJ <- matrix(0,n,n) # create a matrix with n rows and n columns and all values 0
```

Add ties $1 \rightarrow 2$, $1 \rightarrow 3$, $2 \rightarrow 3$,
$3 \rightarrow 4$, and , $4 \rightarrow 5$

```{r adjfill, results='hide'}
ADJ[1,2] <- 1
ADJ[1,3] <- 1
ADJ[2,3] <- 1
ADJ[3,4] <- 1
ADJ[4,5] <- 1
ADJ
```

To make the network **undirected**, add the ties $2 \rightarrow 1$,
$3 \rightarrow 1$, $3 \rightarrow 2$, $4 \rightarrow 3$, and
$5 \rightarrow 4$

```{r adjfill2}
ADJ[2,1] <- 1
ADJ[3,1] <- 1
ADJ[3,2] <- 1
ADJ[4,3] <- 1
ADJ[5,4] <- 1
ADJ
```

#### Cells in the adjacency matrix and tie-variables

In general the cell `r 'ADJ[i,j]'` corresponds to the tie-variable
$X_{i,j}$. Here $x_{1,2}=1$

```{r tievar, results='hide'}
ADJ[1,2]
```

but, for example, $x_{1,4}=0$

```{r tievar2, results='hide'}
ADJ[1,4]
```

The ties of node $i=1$ is the $i$'th row

```{r outties, results='hide'}
ADJ[1,]
```

#### Density

The adjcenacy matrix has

```{r numcells, results='hide'}
dim(ADJ)
```

rows and columns. This means that there are $n \times n$ cells in the
adjacency matrix.

```{r numcells2, results='hide'}
dim(ADJ)[1]*dim(ADJ)[2]
length(ADJ)
```

The $n$ diagonal elements $x_{11},x_{22},\ldots,x_{nn}$ are zero by
definition, which means that there are $n \times n - n = n(n-1)$
variables that can be non-zero, here

```{r numcells3, results='hide'}
dim(ADJ)[1]*dim(ADJ)[2] - n
```

> **Density**: How many variables are equal to 1 out of the total
> posible?

The total number of ones
$$L = \sum_{i,j,i\neq j}x_{ij}=x_{12}+\cdots+x_{1n}+x_{21}+\cdots+x_{(n-1)n}$$
is simply a count of the number of non-zero entries

```{r numties, results='hide'}
sum(ADJ)
```

The density thus is

```{r density, results='hide'}
sum(ADJ)/(n*(n-1))
```

and 50% of possible ties are present in the network.

#### Degree

> How many ties does a node have?

The **degree** $d_i$ of a node $i$ is defined as the sum
$d_i=\sum_{j}x_{i,j}=x_{i,2}+x_{i,2}+\cdots + x_{i,n}$. The degree of
node $i=1$ is thus

```{r deg1, results='hide'}
sum(ADJ[1,])
```

and the degree of node $i=2$ is

```{r deg2, results='hide'}
sum(ADJ[2,])
```

#### Degree distribution

Calculate the *column sum* of the adjacency matrix to get the vector of
degrees (note the capital S)

```{r degs, results='hide'}
colSums(ADJ)
```

The **degree distribution** is the table of frequencies of degrees

```{r degdist, results='hide'}
table( colSums(ADJ) )
```

You can chart the degree distribution with a bar chart

```{r degdistchart,results='hide'}
plot( table( colSums(ADJ) ))
```

> You can use standard R-routines to explore the adjacency matrix

For example finding what node (-s) have, say, degree 3

```{r which, results='hide'}
which(colSums(ADJ)==3)
```

Or subsetting the adjacency matrix to look only at nodes with degree 2
or greater

```{r subset, results='hide'}
use <- which(colSums(ADJ)>=2) # for each row there will be a logical TRUE or FALSE
ADJ[use,use]
```

#### Fun Fact: Linear algebra

Most network metrics can be calculated using linear algebra. For
example, if $X_{i,j}$ in $X$ tell you if $i$ and $j$ are directly
connected, element $(XX)_{i,j}$ of the matrix product $XX$, tells you
how many paths $i \rightarrow k \rightarrow j$ there are

```{r 2path, results='hide'}
ADJ %*% ADJ
```

Element $(XXX)_{i,j}$ of the matrix product $XXX$, tells you how many
paths $i \rightarrow k \rightarrow h \rightarrow j$ there are

```{r 3path, results='hide'}
ADJ %*% ADJ %*% ADJ
```

------------------------------------------------------------------------

### Network object

Plotting the `r 'matrix'` object `r 'ADJ'` is not meaningful because R
does not know that this is an adjacency matrix. To interpret `r 'ADJ'`
as a network, translate the adjacency matrix to a `r 'network'` object

```{r asnetwork, results='hide'}
net <- as.network(ADJ, directed = FALSE)
```

NB: in the `r 'network'` package you use `r 'directed=FALSE'` in lieu of
setting `r 'mode'` equal to `r 'graph'`.

The new object `r 'net'` is an object of type

```{r netobj, results='hide'}
class(net)
```

While printing `r 'ADJ'` to screen just gives you the matrix, priniting
`r 'net'` gives you a summary of the network

```{r netsummary, results='hide'}
net
```

#### Plot sociogram

When plotting a `r 'network'` object, R knows that you want to plot the
**sociogram**

```{r plotnet,results='hide',fig.show = 'hide'}
plot( net )
```

For various plotting option see `r '?plot.network'`. For example, set
node-size to degree, include labels, and set different colours

```{r plotnet2,results='hide'}
plot( net , # the network object
      vertex.cex = degree(net) , # how should nodes (vertices) be scaled
      displaylabels =  TRUE, # display the labels of vertices
      vertex.col = c('red','blue','grey','green','yellow'))
```

Note that `r 'degree(net)'` is a built-in function in `r 'network'` for
calculating the degrees of the nodes. The next step will explore more of
these functions.

## Network position

Continue with the network that we constructed previously in Minimal
Intro.

Recall, basic properties of this network are

```{r summarynet}
net
```

### Degree centrality

> What nodes have more ties?

```{r degredent, results='hide'}
degree( net )
```

This is vector with the degree centrality for each node in the network.
Degree centrality is also called Freeman degree centrality
[@freeman1978centrality].

### Betweeness centrality

Betweeness is related to connectivity and flow in a network
[@freeman1978centrality; @borgatti2006graph]. This measure requires that
you know the network structure of the entire network.

```{r between, results='hide'}
betweenness( net , gmode ='graph' )
```

Node 3 is on the *shortest path* between 4 pairs of nodes:

| Start node | Step 2 | Step 3 | Step 4 |
|:----------:|:------:|:------:|:------:|
|     1      | **3**  |   4    |        |
|     1      | **3**  |   4    |   5    |
|     2      | **3**  |   4    |        |
|     2      | **3**  |   4    |   5    |

> These nodes will only be able to 'communicate' via 3

### Brokers

A cutpoint is a node that when removed results in the network having
more components. In simpler terms, a cutpoint is a node that connects
otherwise dissconnected nodes

```{r cutpoint}
cutpoints( net , mode = "graph")
```

The extent to which nodes broker between other nodes has been the fucus
of a large part of Ron Burt's career. He proposes some elaborations for
measuring bokerness in @burt2009structural called *constraint* and
*effective size*. These are *local* measures that do not, as opposed to
cutpoints and betweeness, require that you know the network structure of
the entire network.

------------------------------------------------------------------------

### Triad census

The triad census tabulates different types of triangles or tripples
[@davis1967structure; @hollandMAN]

| 0 edges          | 1 edge           | 2 edges          | 3 edges          |
|------------------|------------------|------------------|------------------|
| $\sharp$ tripels | $\sharp$ tripels | $\sharp$ tripels | $\sharp$ tripels |

```{r triadc}
triad.census( net , mode = "graph")
```

There are 6 tripples, with one tie. For example the *subgraph*
consisting of nodes 1,4 , and 5.

```{r ptriad1,results='hide'}
plot(as.network( ADJ[c(1,4,5),c(1,4,5)] , directed = FALSE),
     vertex.cex = degree(net)[c(1,4,5)],
     vertex.col = c('red','blue','grey','green','yellow')[c(1,4,5)])
```

**Open triad**: There are 3 tripples, with two ties. For example the
*subgraph* consisting of nodes 3, 4 , and 5.

```{r ptriad2,results='hide'}
plot(as.network( ADJ[c(3,4,5),c(3,4,5)] , directed = FALSE),
     vertex.cex = degree(net)[c(3,4,5)],
     vertex.col = c('red','blue','grey','green','yellow')[c(3,4,5)])
```

**Closed triad**: There is 1 tripple, with three ties. For example the
*subgraph* consisting of nodes 1, 2 , and 3.

```{r ptriad3,results='hide'}
plot(as.network( ADJ[c(1,2,3),c(1,2,3)] , directed = FALSE ),
     vertex.cex = degree(net)[c(1,2,3)],
     vertex.col = c('red','blue','grey','green','yellow')[c(1,2,3)] )
```

### Cliques

A $k$-clique is a maximally connected subgraph. This means that a
$k$-clique is a network with $k$ nodes that are all connected to each
other. In `r 'net'` nodes 5 and 4 consitute a 2-clique and nodes 1, 2,
and 3 constitute a 3-clique. Like the triad census, we can calulate a
clique census for a graph

```{r cliquec, messages=FALSE,results='hide'}
cc <- clique.census( net , mode = "graph")
```

The object `r 'cc'` is of class

```{r cliqueclass, results='hide'}
class(cc)
```

The list has the following objects

```{r cliquenames, results='hide'}
names(cc)
```

The object `r 'clique.count'` lists the membership of nodes in cliques

```{r cliqumem, results='hide'}
cc$clique.count
```

A list of lists of cliques and their members is provided in

```{r cliques, results='hide'}
cc$cliques
```

Note that subgraphs of cliques are not listed. For example, the
2-cliques with nodes 1 and 2 is not listed as both are part of the
larger 3-clique.

## Position directed

This is an introduction to anylisng **directed** networks in R using the
packages `r 'sna'` and `r 'network'`. We will revisit the *adjacency
matrix* and the notion of *tie variables* for directed networks.

We will learn the basic

-   differences between in- and out-degree
-   definitions of the basic subgraphs
    -   dyads
    -   triads

We will use a dataset of 73 high school pupils collected by James
@coleman1964introduction that comes with the package \``r 'sna'`.

For full details of the packages, see
<https://cran.r-project.org/web/packages/sna/sna.pdf> and
<https://cran.r-project.org/web/packages/network/network.pdf>.

The background of, and description, of the dataset is provided in the
helpfile

```{r coledat, eval=FALSE}
?coleman
```

Now load the network

```{r loadcoldat, results='hide'}
data(coleman)
```

### The adjacency matrix

> What do I have in my workspace and what did I load?

The function `r 'data()'` loaded something. Use the general purpose
command `r 'ls()'` to list what is in your workspace

```{r workspace, results='hide'}
ls()
```

This is not one adjacency matrix but

```{r classcol, results='hide'}
class(coleman)
```

As described in the help file this is an array with 2 $\times$ adjacency
matrices of dimensions $73 \times 73$

```{r dimscol, results='hide'}
dim(coleman)
```

The individual slices are matrices

```{r matcol, results='hide'}
class(coleman[1,,])
class(coleman[2,,])
```

You can print `r 'coleman[1,,]'` and `r 'coleman[2,,]'` to screen to
view the adjacency matrices (but that will just be a lot of ones and
zeros, in particular $2 \times 73 \times 73$ ones and zeros). You can
visualise the adjacency matrices in these matrix plots

```{r plotmatr,results='hide'}
par( mfrow = c(1,2))
plot.sociomatrix( coleman[1,,] , drawlab=FALSE , drawlines = FALSE, xlab = 'FALL')
plot.sociomatrix( coleman[2,,] , drawlab=FALSE , drawlines = FALSE, xlab = 'SPRING')
```

In the adjacency matrix, rows record the ties *sent*, and columns record
the ties *received*. Student 1 has the out-tie variables
$x_{1,2},x_{1,3},\ldots,x_{1,n}$

```{r outtiesS1, results='hide'}
 coleman[1,1,]
```

For example, that `r 'coleman[1,1,14]'` is 1 means that student 1 has a
tie to student 14, and that `r 'coleman[1,1,2]'` is 0 means that student
1 does not have a tie to student 2.

#### Outdegree

Taking the sum of the outties

```{r outdeg1, results='hide'}
 sum(coleman[1,1,])
```

gives us student 1's **outdegree**, $d_i^o= \sum_{j,j\neq i}x_{i,j}$.

To get the outdegree of all pupils, sum across columns for all rows

```{r outdegs, results='hide'}
 rowSums(coleman[1,,])
```

And, like for un-directed networks, we can chart the the degree
distribution

```{r outdegdis,results='hide',fig.show = 'hide'}
 plot( table (rowSums(coleman[1,,]) ))
```

#### Indegree

The ties a student has received is the column of that particular
student. Student 1 has received the tie variables
$x_{2,1},x_{3,1},\ldots,x_{n,1}$

```{r inties, results='hide'}
 coleman[1,,1]
```

For example, that `r 'coleman[1,14,1]'` is 0 means that student 1 has
not received a tie *from* student 14.

To get the **indegree** of all pupils sum across rows for all columns

```{r indegs, results='hide'}
 colSums(coleman[1,,])
```

And, like for un-directed networks, we can chart the the indegree
distribution

```{r indegdis, results='hide',results='hide'}
 plot( table (colSums(coleman[1,,]) ))
```

> The number of ties sent and received are not the same!

For example, student 1 has nominated 5 other people but student 1 has
not been nominated back at all. Plotting the outdegrees against the
indegrees

```{r outvin, results='hide',results='hide',fig.show = 'hide'}
 plot( jitter(rowSums(coleman[1,,]) ) , jitter( colSums(coleman[1,,]) ) ) 
```

reveal that some send more than they receive, and some send fewer than
they receive.

One reason for this is that some pairs are asymmetric and other pairs
are symmetric in their choices [there are of course other possible
explanations @igarashi2020overchoosing]

### Dyads and reciprocity

A **dyad** is a pair of nodes and their ties $(X_{i,j}X_{j,i})$ to
each other. To investigate whether pairs are symmetric or asymmetric,
consider e.g. that 1 nominated 14 but 14 did not nominate 1 back - this
pair is asymmetric.

```{r assymex, results='hide'}
 coleman[1,1,14]
 coleman[1,14,1]
```

The dyad 19 and 4

```{r mutex, results='hide'}
 coleman[1,4,19]
 coleman[1,19,4]
```

is symmetric, or **reciprocal** - we call this a **mutual dyad**.

In general we can distinguish between dyads that are *Mutual*,
*Assymetric*, and *Null* [@hollandMAN]:

```{r MANplot,results='hide'}
par ( mfrow = c(1,3))
plot( as.network(coleman[1,c(4,19),c(4,19)] ),main='Mutual' ,label = c(4,19), usecurve = TRUE, edge.curv = 0.01, arrowhead.cex = 2.5, vertex.cex = degree(coleman[1,,])[c(4,19)] ) 
plot( as.network(coleman[1,c(1,14),c(1,14)] ),main='Assymetric' ,label = c(1,14), arrowhead.cex = 2.5, vertex.cex = degree(coleman[1,,])[c(1,14)] ) 
plot( as.network(coleman[1,c(1,2),c(1,2)] ),main='Null' ,label = c(1,2) , vertex.cex = degree(coleman[1,,])[c(1,2)] ) 
```

#### Fun Fact: Linear algebra

Recall that the inner product of a matrix $X$ with itself $X$ has
entries $(XX)_{i,j}=\sum_{k}X_{i,k} X_{ k,j }$. Consequently,
$\mathrm{tr}(XX)$ gives you twice the number of reciprocated dyads.
There is no standard command for trace in R, but you can take the
`r 'sum()'` of the diagonal `r 'diag()'` of the matrix product, e.g. for
adjacency matrix `r 'A'`, `r 'sum( diag( A %*% A ))'`. The standard
product `r '*'` in R is the dot product $X \odot X^{\top}$ where
$(X \odot X^{\top})_{i,j}=X_{i,j} X_{ i,j }$. Consequently for adjacency
matrix `r 'A'`, `r 'A * t(A)'` is the matrix of reciprochated ties.

### Dyad census

The **dyad census** tabulates the number of dyads that are Mutual,
Assymetric, and Null

```{r dyadcens}
 dyad.census(coleman[1,,])
```

#### Fun Fact: census is complete enumeration of dyads

Note that in the Coleman example, the total number of M, A, and N dyads
sum up to

```{r totdyadcens, results='hide'}
 62 + 119 +  2447
```

which is exactly equal to $n(n-1)/2=73\times72/2=2628$ - this is the
total number on (un-ordered) pairs, the number of cells in the upper
diagonal of the adjacency matrix.

### Triads

For tripplets in directed networks, there many different kinds. For
*open triads*, we could for example consider the three different types
you can achieve with 0 Mutual dyads, 2 Assymetric dyads, and 1 Null
dyad.

```{r plotopen,results='hide'}
par( mfrow = c(1,3))
plot( as.network( matrix(c(0,1,0,0,0,1,0,0,0),3,3 ,byrow=TRUE) ), coord = matrix(c(0,0,5,10,10,0) ,3,2,byrow=TRUE) ,arrowhead.cex = 2.5, vertex.cex =3 , jitter = FALSE, main='021C')
plot( as.network( matrix(c(0,1,0,0,0,0,0,1,0),3,3 ) ), coord = matrix(c(0,0,5,10,10,0) ,3,2,byrow=TRUE) ,arrowhead.cex = 2.5, vertex.cex =3 , jitter = FALSE, main='021D')
plot( as.network( matrix(c(0,0,0,1,0,1,0,0,0),3,3 ) ), coord = matrix(c(0,0,5,10,10,0) ,3,2,byrow=TRUE) ,arrowhead.cex = 2.5, vertex.cex =3 , jitter = FALSE,  main='021U')
```

The triples are labeled by the number of Mutual, Assymetric, and Null
dyads - the so-called MAN labeling scheme. Here, the 'C', 'D', and 'U'
distinguish between 'Cyclic', 'Down', and 'Up'.

For closed triands consider the Transitive ('T'), Cyclic ('C'), and
complete closed triads

```{r plotclosed,results='hide'}
par( mfrow = c(1,3))
plot( as.network( matrix(c(0,1,0,0,0,1,1,0,0),3,3 ,byrow=TRUE) ), coord = matrix(c(0,0,5,10,10,0) ,3,2,byrow=TRUE) ,arrowhead.cex = 2.5, vertex.cex =3 , jitter = FALSE, main='030C')
plot( as.network( matrix(c(0,1,1,0,0,0,0,1,0),3,3 ) ,byrow=TRUE), coord = matrix(c(0,0,5,10,10,0) ,3,2,byrow=TRUE) ,arrowhead.cex = 2.5, vertex.cex =3 , jitter = FALSE, main='030T')
plot( as.network( matrix(c(0,1,1,1,0,1,1,1,0),3,3 )  ,byrow=TRUE), coord = matrix(c(0,0,5,10,10,0) ,3,2,byrow=TRUE) ,arrowhead.cex = 2.5, vertex.cex =3 , jitter = FALSE,  main='300', usecurve = TRUE, edge.curv = 0.01)
```

When considering the potential interpretation of these closed triads,
imagine scenarios where ties reflect 'liking', 'look up to', 'give
orders', 'passing on information', and how these structures would be
reflected in status, chain of command, access to information, etc.

### Triad census

In total there are 16 types of triangles, all of which are labelled
using the MAN labeling scheme [@hollandMAN]. For the Coleman data, we
calulate the **triad census** as

```{r triadcens}
 triad.census(coleman[1,,])
```

#### Fun Fact: census is complete enumeration of triads

Note that in the Coleman example, the total number triads

```{r tottriadcens, results='hide'}
sum(triad.census(coleman[1,,]))
```

which is exactly equal to
$$\binom{n}{3}= \frac{n(n-1)(n-1)}{(3\times 2)}=73\times72\times71/6=62196$$ -
this is the total number on (un-ordered) tripplets, the number of ways
in which you can chose 3 element subsets out of a 73 element set.

### Plotting the network

Plotting the sociogram of the network does not differ from the
undirected case, with the exception that the ties are now represented by
arrows.

```{r plotcole,results='hide'}
plot( as.network( coleman[1,,] , directed= TRUE), # the network object
      vertex.cex = degree(coleman[1,,], cmode = 'indegree')/5 + .2 )
```

The size of a node here is set proportional to the indegree centrality.

> There are two large 'clusters' of nodes that are connected within but
> not between - these are two **components**

# Non-parametric

Using the data we downloaded and formatted in the `Data-Formatting.Rmd`
vignette, we will here investigate how much these (undirected) networks
differ from random networks. Analogously, we might ask what features of
the networks are not guided by chance.

Make sure that you have loaded the required libraries but also load
`ergm()`

```{r loadergm, results='hide', warning=FALSE, message=FALSE}
library("ergm")
```

## Preamble

### Load formatted data

```{r loaddata}
load(url("https://raw.githubusercontent.com/johankoskinen/CHDH-SNA/main/data/undirected.RData"))
ls()#list all the objects you downloaded
```

> The object `net.profiles` contains a number of formatted networks
> along with some network summary measures

```{r}
names(net.profiles)
```

### Some notation

We let the set of $n$ nodes be $V = \{1,\ldots,n \}$, and we define the
symmetric adjacency matrix $X=(X_{ij})_{ij \in V^{(2)}}$, where the
non-redundant pairs are $\mathcal{N} = { V \choose 2}$, with elements

$$ 
X_{ij}= \left\{
\begin{array}{lr}
    1,&\text{if there is a tie from } i \text{ to } j, i,j \in V\\
    0,&\text{else}
\end{array} 
\right. {\text{,}}
$$

## Completely random graph

Assume that we independently for each pair, $\{i,j\}$, decided if the
tie existed by tossing a fair coin. This means that, *independently* for
all $\{i,j\} \in \mathcal{N}$ $$
\Pr (X_{ij}=1) = 0.5
$$ In `r 'sna'`, the function `r 'rgraph'` generates random graphs.

### Padget's business network

Generate a $n=16$ completely random graph and compare it to the
@padgett1993robust Florentine families business network

#### Generate random network

We will generate a network that only has $n$ in common with the Padgett
data set.

```{r padgettgit, results='hide'}
row.index <- which(net.profiles$netnames=='PadgetB')# the row in the matrix containing Padgett summaries
n <- net.profiles$net.size[ row.index ]
g <- rgraph(n,#size of the network
            m=1,# number of networks to generate
            tprob=0.5,# tie probability
            mode="graph") # undirected (graph) or directed (digraph)
g.net <- as.network(g,directed=FALSE)# 'sna' give you a matrix
gden( padgbus.net )# observed density
gden(g.net)# density 
```

Now we can compare what a network would look like if ties were formed
completely at random with our observed network.

#### Plot and compare

```{r plotpadgettgit}
par(mfrow=c(1,2))
plot(padgbus.net)
plot(g.net)
```

> Does it look more 'random' than the Padgett network?

This is only *one* random network, maybe it looks different by chance.
Generate $m = 100$ networks

```{r randompadgett, results='hide'}
row.index <- which(net.profiles$netnames=='PadgetB')# the row in the matrix containing Padgett summaries
n <- net.profiles$net.size[ row.index ]
g100 <- rgraph(n,#size of the network
            m=100,# number of networks to generate
            tprob=0.5,# tie probability
            mode="graph") # undirected (graph) or directed (digraph)
```

#### Calculate metrics

Any metric that we calculate for the *observed* network we can calculate
for a *random* network

```{r padgetstructsrgraph, results='hide',message=FALSE}
par( mfrow  = c(1,3) )
hist( gden( g100) , # the density for each of the 100 random networks
      xlim=range( gden( g100 ), 
                  gden( padgbus.net ) ) , main='density') 
abline( v=gden( padgbus.net ),col='red' )# the observed density for reference

row.index <- which(net.profiles$netnames=='PadgetB')# the row in the matrix containing Padgett summaries
degrees <-degree(g100,g=c(1:100), cmode='indegree')# you can calculate the degree distributions for all graphs
deg.sist <- cbind( matrix( colSums(degrees==0),100,1),
                  t(apply(degrees,2,function(x) tabulate(x, nbins=max.deg) ) ) )# when tabulating we need to add the number of isolates 
matplot(c(0:(max.deg)), t(deg.sist) ,
        type ='l',
        main='degree distribution' )
lines(c(0:(max.deg)),degree.dist[row.index,c(2:(max.deg+2))],col='grey',lwd=3)

triads <- triad.census( g100 , mode='graph')[,4]# CHANGE
hist(triads ,
     xlim= range( triads , net.profiles$net.closed.triad[ row.index ] ),
     main = 'closed triads')
abline( v=net.profiles$net.closed.triad[ row.index ],col='red' )# the observed density for reference

```

Clearly, since the random networks are *too dense*, the degree
distributions are also different from the observed network, with nodes
on average having a higher degree. But

> despite how dense the networks are, there are too many triangles but
> is that because we have so many more ties???

#### Fixed density: implications

Will a random network with density $0.5$ resemble any networks?

Plot the densities for the observed networks as a function of size

```{r structuresum}

plot( net.profiles$net.size[order(net.profiles$net.size)],
      net.profiles$net.dens[order(net.profiles$net.size)],
      type='b',
      xlab = 'network size',ylab = 'density',ylim=range(net.profiles$net.dens,.55))
abline(h=0.5, col='red')# reference line for the completely random graphs

```

> fixing the density at 0.5 irrespective of network size give
> unrealisically dense networks

## Density conditioned graph

If we do not get density right, we won't get anything righ. We introduce
a random graph that fixes the density.

### Definition

Let $L(x) = \sum_{i<j}x_{ij}$ be the number of edges in a graph. The
number of graphs with exactly $k$ edges
$$
\mathcal{X}_k= \{ x \in \mathcal{X} : L(x)=k \}
$$
is given by ${ M \choose k}$, where $M= { n \choose 2}$, and
consequently the conditional distribution
$$
\Pr(X=x \mid L(x)= k ) = \frac{1}{{ M \choose k}}
$$


Note that are no longer deciding independently for each pair if there is
a tie or not, rather we select $k$ pairs at random from the total set of
pairs and decide that they will have tie.

We refer to this model as $X \thicksim U \mid L(X)= k$.

In `sna`, the function `rgnm` generates random graphs with
exactly the same number of ties as the observed network (same density)

### Florentine families

#### Simulate graphs


```{r crandompadgett, results='hide',message=FALSE}
row.index <- which(net.profiles$netnames=='PadgetB')# the row in the matrix containing Padgett summaries
 
cg100 <- rgnm(n =100,# number of networks to generate
            nv=net.profiles$net.size[ row.index ],#size of the network
            m = net.profiles$net.ties[ row.index ],# number of edges
            mode="graph") # undirected (graph) or directed (digraph)
```


#### Plot random

Plot the observed network and *one* of the simulated networks

```{r uplotpadgettgit}
par(mfrow=c(1,2))
plot(padgbus.net)
plot(as.network(cg100[1,,],directed=FALSE ))
```

> Does the random graph look more like the empirical graph?

#### Calculate metrics on random networks

Any metric that we calculate for the *observed* network we can calculate
for a *random* network. Calculate density, degree distribution, and
triangles for all 100 networks and plot and compare

```{r cpadgetstructsrgraph}
par( mfrow  = c(1,3) )
hist( gden( cg100) , # the density for each of the 100 random networks
      xlim=range( gden( cg100 ), 
                  gden( padgbus.net ) ) , main='density') 
abline( v=gden( padgbus.net ),col='red' )# the observed density for reference

row.index <- which(net.profiles$netnames=='PadgetB')# the row in the matrix containing Padgett summaries
degrees <-degree(cg100,g=c(1:100), cmode='indegree')# you can calculate the degree distributions for all graphs
deg.sist <- cbind( matrix( colSums(degrees==0),100,1),
                  t(apply(degrees,2,function(x) tabulate(x, nbins=max.deg) ) ) )# when tabulating we need to add the number of isolates 
matplot(c(0:(max.deg)), t(deg.sist) ,
        type ='l',
        main='degree distribution' )
lines(c(0:(max.deg)),degree.dist[row.index,c(2:(max.deg+2))],col='grey',lwd=3)

triads <- triad.census( cg100 , mode='graph' )[,4]
hist(triads ,
     xlim= range( triads , net.profiles$net.closed.triad[ row.index ] ),
     main = 'closed triads')
abline( v=net.profiles$net.closed.triad[ row.index ],col='red' )# the observed density for reference

```

The density is constant but

> The degree distributions have completely different shape to the
> observed (grey) and few of the simulated networks have many triangles

### Other example

```{r names}
netnames
```

```{r otherexamplepick, results='hide'}
# Pick one
net <- tribesPos.net# to simply rename temporarily
row.index <- which(net.profiles$netnames=='TribesPos')# the row in the matrix containing Padgett summaries
```

#### Simulate graphs

```{r otherexample, results='hide'}
cg100 <- rgnm(n =100,# number of networks to generate
            nv=net.profiles$net.size[ row.index ],#size of the network
            m = net.profiles$net.ties[ row.index ],# number of edges
            mode="graph") # undirected (graph) or directed (digraph)
```

#### Plot random

Plot the observed network and *one* of the simulated networks

```{r uplotother}
par(mfrow=c(1,2))
plot(net)
plot(as.network(cg100[1,,],directed=FALSE ))
```

#### Calculate metrics

Calculate density, degree distribution, and triangles for all 100
networks and plot and compare

```{r ctribesstructsrgraph, results='hide',message=FALSE}
par( mfrow  = c(1,3) )
hist( gden( cg100) , # the density for each of the 100 random networks
      xlim=range( gden( cg100 ), 
                  gden( net ) ) , main='density') 
abline( v=gden( net ),col='red' )# the observed density for reference

degrees <-degree(cg100,g=c(1:100), cmode='indegree')# you can calculate the degree distributions for all graphs
deg.sist <- cbind( matrix( colSums(degrees==0),100,1),
                  t(apply(degrees,2,function(x) tabulate(x, nbins=max.deg) ) ) )# when tabulating we need to add the number of isolates 
matplot(c(0:(max.deg)), t(deg.sist) ,
        type ='l',
        main='degree distribution' )
lines(c(0:(max.deg)),degree.dist[row.index,c(2:(max.deg+2))],col='grey',lwd=3)

triads <- triad.census( cg100 , mode='graph' )[,4]
hist(triads ,
     xlim= range( triads , net.profiles$net.closed.triad[ row.index ] ),
     main = 'closed triads')
abline( v=net.profiles$net.closed.triad[ row.index ],col='red' )# the observed density for reference

```

### For all graphs

The conditional uniform $X \thicksim U \mid L(X)= k$ distribution does
not seem to do a good job of clustering. Let us check for all of our
networks prepared in `Data-Formatting.Rmd`:

```{r simulatemany}
par(mfrow=c(1,3))
BigT <- apply(net.profiles[,c(2,3)],1,function(x) {
  triad.census( rgnm(n =30,# number of networks to generate
            nv=x[1],#size of the network
            m = x[2],# number of edges
            mode="graph") # undirected (graph) or directed (digraph)
  , mode="graph")[,4]# important 'graph' giving the 4 triads
  })



boxplot(BigT[,order(net.profiles$net.size)],#the raw triad counts ordered by network size
        ylim=range(BigT,net.profiles$net.closed.triad),
        main='triangles',clab='network',ylab='raw counts')
lines(net.profiles$net.closed.triad[ order(net.profiles$net.size) ],type='b',col='red')

ave.T <- colMeans(BigT)
BigT.norm <- apply(BigT,2, function(x) x/mean(x))# normalize

boxplot(BigT.norm[,order(net.profiles$net.size)],# the normalised triad counts ordered by network size
        ylim=range(BigT.norm,net.profiles$net.closed.triad/ave.T ),
        main='triangles',clab='network',ylab='centered counts')
lines(net.profiles$net.closed.triad[order(net.profiles$net.size)]/ave.T[order(net.profiles$net.size)] ,type='b',col='red')




BigC <- apply( net.profiles[,c(2,3)],1,function(x) {
  centralization( rgnm(n =30,# number of networks to generate
            nv=x[1],#size of the network
            m = x[2],# number of edges
            mode="graph") # undirected (graph) or directed (digraph)
  , degree, mode="graph")
  })
boxplot(BigC[,order(net.profiles$net.size)],# the centrality scores ordered by network size
        ylim=range(BigC,net.profiles$net.centralisation),ylab='frequency',xlab='network',main='Centralization')
lines(net.profiles$net.centralisation[order(net.profiles$net.size)],# add observed value for reference
      type='b',col='red')
```

> Clearly, density is **not** the only thing that matters in
> tie-formation because no simulated networks produce enough triangles

## Erdos-Reyini

### Definition

The Bernoulli graph assumes that *independently* for each dyad
$\{i,j\} \in \mathcal{N}$, a tie is formed with probability
$p_{ij} \in (0,1)$ $$
\Pr(X_{ij}=1 ) = p_{ij}
$$ It is a *homogenous* Bernoulli graph, or Erdos-Reyni graph, if
$p_{ij}=p$ for all $\{i,j\} \in \mathcal{N}$. We say that $$
X \thicksim Bern(p)
$$ for a homogenous Bernoulli model with tie-probability $p$.

A particular form of Bernoulli graphs are stochastic blockmodels, where
the tie-probability for $\[ i,j \}$ depends on class memberships of the
nodes $i$ and $j$.

### Padget's business network

Assuming that $X \thicksim U \mid L(X)= k$ did produce enough triangles
and gave the wrong shape for the degree distribution. Assuming
$X \thicksim Bern(p)$, the average density of graphs will be equal to
that of the observed network $x_{\mathrm{obs}}$ if $$
p= \frac{L(x_{\mathrm{obs}})}{M}
$$ but the variance in $L(X)$ will not be zero as in
$X \thicksim U \mid L(X)= k$.

> Will introducing more variability produce more triangles and more
> skewed degree distriubions?

#### Simulate graphs

```{r berrandompadgett, results='hide'}
row.index <- which(net.profiles$netnames=='PadgetB')# the row in the matrix containing Padgett summaries
ber100 <- rgraph(net.profiles$net.size[ row.index ],#size of the network
            m=100,# number of networks to generate
            tprob=net.profiles$net.dens[ row.index ],# tie probability equal to observed density
            mode="graph") # undirected (graph) or directed (digraph)
```

#### Plot random

Plot the observed network and *one* of the simulated networks

```{r berplotpadgettgit}
net <- padgbus.net# to simply rename temporarily
par(mfrow=c(1,2))
plot(net)
plot(as.network(ber100[1,,],directed=FALSE ))
gden(net)# observed density
gden(ber100[1,,])# density of a simulated network
```

> Does the random graph look more like the empirical graph?

#### Calculate metrics on random networks

Any metric that we calculate for the *observed* network we can calculate
for a *random* network. Calculate density, degree distribution, and
triangles for all 100 networks and plot and compare

```{r berpadgetstructsrgraph}
par( mfrow  = c(1,3) )
hist( gden( ber100 ) , # the density for each of the 100 random networks
      xlim=range( gden( ber100 ), 
                  gden( net ) ) , main='density') 
abline( v=gden( net ),col='red' )# the observed density for reference


degrees <-degree(ber100,g=c(1:100), cmode='indegree')# you can calculate the degree distributions for all graphs
deg.sist <- cbind( matrix( colSums(degrees==0),100,1),
                  t(apply(degrees,2,function(x) tabulate(x, nbins=max.deg) ) ) )# when tabulating we need to add the number of isolates 
matplot(c(0:(max.deg)), t(deg.sist) ,
        type ='l',
        main='degree distribution' )
lines(c(0:(max.deg)),degree.dist[row.index,c(2:(max.deg+2))],col='grey',lwd=3)

triads <- triad.census( ber100 , mode='graph' )[,4]
hist(triads ,
     xlim= range( triads , net.profiles$net.closed.triad[ row.index ] ),
     main = 'closed triads')
abline( v=net.profiles$net.closed.triad[ row.index ],col='red' )# the observed density for reference

```

> The degree distributions are more forgiving (but all have wrong shape)
> but still too few triangles

To get an idea just of how unlikely it is that a Bernoulli graph *could
have* produced $5$ triangles (as in Padgett's data) or more is

```{r ovalpadg}
mean( triads >= 5 )
```

```{r berpadgdist, warning = FALSE}

distances <- apply(ber100,
                          1, 
                          function(x) tabulate( geodist( x )$gdist[upper.tri(geodist( x )$gdist)] , nbins=10 ) )  
                    
 
matplot(c(1:(10)), distances ,
        type ='l',
        main='geodesic distribution' ,
        xlab='distance',
        ylab='frequency')
lines(c(1:(10)),
      tabulate( geodist( net )$gdist[upper.tri(geodist( net )$gdist)] , nbins=10 ),
      pch=24,bg="blue",lty=1,type='b',
      col='red',lwd=3 )


```

The model $X \thicksim Bern(p)$ does great in capturing the distances -
Bernoulli graphs have **short pathlengths**. It achieves this by
sacrifing clustering though. If there is a choice between 'linking to
someone new' or 'linking back to someone you indirectly know', it choses
the new one - because ties form idependenlty, the Bernoulli
tie-probability does not know whom a node already has ties to.

> Bernoulli graphs do not waste ties on clustering - this creates reach

Interpreting this in terms of tie-formation processes:

> Independence means that a tie is more likely to a 'new' person than to
> 'a friend of a friend' because Bernoulli does not know (independence)
> whom your friends already are

Clearly, we cannot assume that people form ties independently of each
other!

### For all graphs

Let us see how $X \thicksim Bern(p)$ does for all networks.

```{r bersimulatemany}

par(mfrow=c(1,3))
BigT <- apply(net.profiles[,c(2,4)],1,function(x) {
  triad.census( rgraph(n = x[1],
                       m =30,# number of networks to generate
                       tprob= x[2], # density of network
                       mode="graph") # undirected (graph) or directed (digraph)
  , mode="graph")[,4]# important 'graph' giving the 4 triads
  })



boxplot(BigT[,order(net.profiles$net.size)],#the raw triad counts ordered by network size
        ylim=range(BigT,net.profiles$net.closed.triad),
        main='triangles',clab='network',ylab='raw counts')
lines(net.profiles$net.closed.triad[ order(net.profiles$net.size) ],type='b',col='red')

ave.T <- colMeans(BigT)
BigT.norm <- apply(BigT,2, function(x) x/mean(x))# normalize

boxplot(BigT.norm[,order(net.profiles$net.size)],# the normalised triad counts ordered by network size
        ylim=range(BigT.norm,net.profiles$net.closed.triad/ave.T ),
        main='triangles',clab='network',ylab='centered counts')
lines(net.profiles$net.closed.triad[order(net.profiles$net.size)]/ave.T[order(net.profiles$net.size)] ,type='b',col='red')




BigC <- apply(net.profiles[,c(2,4)],1,function(x) {
  centralization( rgraph(n = x[1],
                       m =30,# number of networks to generate
                       tprob= x[2], # density of network
                       mode="graph")  # undirected (graph) or directed (digraph)
  , degree, mode="graph")
  })
boxplot(BigC[,order(net.profiles$net.size)],# the centrality scores ordered by network size
        ylim=range(BigC,net.profiles$net.centralisation),ylab='frequency',xlab='network',main='Centralization')
lines(net.profiles$net.centralisation[order(net.profiles$net.size)],# add observed value for reference
      type='b',col='red')
```

The extra variability offered by $Bern(p)$ does a little to improve the
features of the simulated networks but much of the clustering and
centralisation remains unexplained.

## Conditional degree distribution

Since it is clear that $Bern(p)$ does not capture the degree
distribution of the typical observed network. This is not surprising
given that the tie-probabilities for all dyads is the same and thereby
for all nodes - all nodes should have on average the same degree.

To see how much of the structure of a network we can explain through the
degree distribution we introduce yet another model.

### Definition

Like the $U \mid L(X)=k$ distribution, we define a distribution for
graphs (not for tie-variables). We let $X \thicksim U \mid X_{\cdot+}=d$
mean that the distribution is uniform on all the graphs that have *the
exact same degree distribution*. Thus
$$ 
\Pr( X = x) = \left\{
\begin{array}{lr}
    c^{-1},&\text{if } x_{\cdot+}=d\\
    0,&\text{else}
\end{array} 
\right. {\text{,}}
$$ 
where $x_{\cdot+}=(\sum_j x_{1j},\ldots,x_{nj})^{\top}$ is the vector
of degrees, and $c = \sharp \{ x \in \mathcal{X}: x_{\cdot+}=d \}$.

There are several algorithms for simulating networks with fixed degree
distribution and all of (the reliable ones) rely on Markov chain Monte
Carlo (MCMC).

#### Note on MCMC

The premise of MCMC is the you want to draw one variable $x$ from a
distribution $p(x)$ but you cannot do that directly. What you can do
though is to say how much more likely one realisation of a variable is
relative to another. For example, let $x$ and $y$ be two networks and
that the *target distribution* is $p(x)$. What we need to know in order
to use MCMC is the ratio $p(x)/p(y)$ for all $x,y \in \mathcal{X}$. This
allows us to generate a sequence of realisations
$$
x_0,x_1,x_2,\ldots,x_K
$$
of outcomes. This is an iterative scheme where you have an updating
rule that takes an outcome $x_t$ and updates it to $x_{t+1}$, and this
update relies on our ability to calculate $p(x)/p(y)$ for all
$x,y \in \mathcal{X}$. In particular, if we are in state $x_t$, and we
consider moving to state $x^{\ast}$, we know how much more or less
likely the new state is relative to the old state $p(x^{*})/p(x_t)$. For
networks, these updates are often incremental and you either set
$x_{t+1}:=x^{\ast}$ or $x_{t+1}:=x_{t}$, depending on $p(x^{*})/p(x_t)$.
Consequently, sometimes we stay in the same state, in the same network,
for many iterations.

Burnin: to make sure that $x_t$ has 'forgotten' the initial state $x_0$,
you usuall allow for a number of interations to pass - this is the
burnin period.

Thinning: If you have an adequate burning and want to draw many
outcomes, you may not have to do the same burnin again, instead you
allow for a certain number of iterations bewtween sucessive sample
points - the number of iterations you wait is called thinning.

### Padget's business network

#### Simulate graphs

```{r unifpadgett, results='hide'}
g.udegs <- simulate(padgbus.net~edges,
                    coef=c(0),
                    constraints=~degreedist,# this is what does it degreedist is based on padgbus.net
                    nsim=100,
                    control=control.simulate(MCMC.burnin=100000))# you need to bump up the default burnin, otherwise the networks are too similar to the starting point, the observed network
```

To confirm that this network actually has the same degree distribution:

```{r unifpadgettdeg}
par(mfrow=c(1,3))
deg1 <- degree(padgbus.net, cmode='indegree')
deg2 <- degree(g.udegs[[5]], cmode='indegree')# this is a list of networks, not an array, hence [[k]]
plot( deg1,deg2 ,xlab='Padgett', ylab ='random network',main='degrees')#
plot( deg1[order(deg1)],deg2[order(deg2)] ,ylab ='random network',main='degrees')# the degree distribution is the same but not the same nodes have the same degrees
plot( table(deg1), type='l',col='grey',lwd=4)
lines( table(deg2) ,col='red' )
```

From the first panel we see that not every node has the same degree in
the random network as in the observed network. However, the degree
distribution is the same, as seen in thee right-hand panel. The middle
panel shows that there is a mapping from the nodes in $x_{\mathbf{obs}}$
to the nodes in $x$, so that if for every node with degree $k$ in the
former, there is exactly one node in the latter that has degree $k$.

### Plot random

Plot the observed network and then three of the random networks

```{r unifpadgettplot}
par( mfrow =c(1,4) )
plot( padgbus.net )
plot( g.udegs[[5]] )
plot( g.udegs[[50]] )
plot( g.udegs[[100]] )
```

> The simulated networks look like pretty faithful represeantions of the
> observed network

Since the degree distribution is identical to the observed network, the
density will also be preserved for all simulated networks. Let us look
at triangles and geodesic distances.

```{r unifpadgetstructsrgraph, warning = FALSE}
par( mfrow  = c(1,3) )
row.index <- which(net.profiles$netnames=='PadgetB')# t

triads <- triad.census( g.udegs , mode="graph")[,4]
hist(triads ,
     xlim= range( triads , net.profiles$net.closed.triad[ row.index ] ),
     main = 'closed triads')
abline( v=net.profiles$net.closed.triad[ row.index ],col='red' )# the observed density for reference

triads <- triad.census( g.udegs ,mode="graph" )[,3]
hist(triads ,
     xlim= range( triads , net.profiles$net.open.triad[ row.index ] ),
     main = 'open triads')
abline( v=net.profiles$net.open.triad[ row.index ],col='red' )# the observed density for reference

# because g.udegs is a list it is a little more complicated to use built in sna functions
distances <- matrix(0,10,100)
for (k in c(1:100))
{
  distances[,k] <- tabulate( geodist( g.udegs[[k]] )$gdist[upper.tri(geodist( g.udegs[[k]] )$gdist)], 
                             nbins=10 )  
}

matplot(c(1:(10)), distances ,
        type ='l',
        main='geodesic distribution' ,
        xlab='distance',
        ylab='frequency')
lines(c(1:(10)),
      tabulate( geodist( padgbus.net )$gdist[upper.tri(geodist( padgbus.net )$gdist)] , nbins=10 ),
      pch=24,bg="blue",lty=1,type='b',
      col='red',lwd=3 )


```

The $X \thicksim U \mid X_{\cdot+}=d$ model does not fully capture
clustering and it over-produces open triangles.

> Even if we could model the degree of every node exactly, we cannot produce the same clustering that we see for real networks

> The degree distribution alone is not sufficient for explaining clustering and reach - people do not only care about how many friends they have

## ERGM

### Definition

An introduction to ERGMs is given in the slides. Formally, an ERGM is a
model
$$
\Pr( X = x \mid \theta) = \frac{ \exp \{ \theta^{\top}z(x)  \}   }{ \sum_{x \in \mathcal{X} } \exp \{ \theta^{\top}z(x) \} }
$$

where \* $\theta$ is a vector of statitical parameters \* $z(x)$ is a
vector of statistics calculated on $x$ \* the denominator is a sum over
all graphs ensuring that the probaility sums to one

#### Example

If $z(x)=L(x)$, the only model statistic is the number of edges in the
graph. This model is equivalent to $Bern(p)$, where
$$
\theta = - \log\left( \frac{1}{p-1}\right)
$$
or, expressed in terms of $p$
$$
p= \frac{e^{\theta L(x)}}{1+e^{\theta L(x)}}
$$

### Properties

A property of ERGM is that you can set the values of the expected
statistics. The expected statistics
$$
E_{\theta} \{ z(X) \} = \sum_{x \in \mathcal{x}} z(x)\Pr( X = x \mid \theta) 
$$
are completely determined by the parameters.

### Padget's business network

#### Bernoulli model

Simulate networks from an ERGM with only the statistic $z(x)=L(x)$ and
paramter $\theta = âˆ’1.945$

```{r bernERGMpadgett, results='hide'}
g.ergm.bern <- simulate(padgbus.net~edges,# one statistic
                    coef= -1.945,# one parameter
                    nsim=100,
                    control=control.simulate(MCMC.burnin=100000))# you need to bump up the default burnin, otherwise the networks are too similar to the starting point, the observed network
```

Inspect networks

```{r ergmberpadgetstructsrgraph}
par( mfrow  = c(1,2) )
hist( gden( g.ergm.bern ) , # the density for each of the 100 random networks
      xlim=range( gden( g.ergm.bern ), 
                  gden( padgbus.net ) ) , main='density') 
abline( v=gden( padgbus.net ),col='red' )# the observed density for reference


triads <- triad.census( g.ergm.bern , mode='graph' )[,4]
hist(triads ,
     xlim= range( triads , triad.census( padgbus.net , mode='graph' )[,4] ),
     main = 'closed triads')
abline( v= triad.census( padgbus.net , mode='graph' )[,4],col='red' )# the observed density for reference

```

The denisty of graphs is centred right over the observed value! Because
this model is equivalent to $Bern(p)$, the missfit of closed triads is
the same as before.

> The Bernoulli ERGM assumes that ties form independently - how can we
> relax that?

#### Markov model

Since the Bernoulli ERGM does not reproduce the number of triangles, how
do can we improve on that?

If we add statistics for the number of triangles we can increase the
number of triangles that the model produces. Let us add statistics:

-   edges: $\sum_{ i <j }x_{ij}$
-   two-stars: $\sum_{ i ,j,k }x_{ij}x_{ik}$
-   three-stars: $\sum_{ i ,j,k,h }x_{ij}x_{ik}x_{ih}$
-   triangles: $\sum_{ i ,j,k}x_{ij}x_{ik}x_{jk}$

We will use the parameters
$\theta = \left( -2.1113 ,1.0465 ,-0.6318, 1.3064 \right)^{\top}$

```{r markovERGMpadgett}
g.ergm.markov <- simulate(padgbus.net~edges+kstar(2) +kstar(3) + triangles,# one statistic
                    coef= c(-4.4878  ,  1.2556 ,  -0.7059  ,  1.0266),# one parameter
                    nsim=100,
                    control=control.simulate(MCMC.burnin=100000))# you need to bump up the default burnin, otherwise the networks are too similar to the starting point, the observed network

```

Inspect networks

```{r markovpadgett}
par( mfrow  = c(2,2) )
hist( gden( g.ergm.markov ) , # the density for each of the 100 random networks
      xlim=range( gden( g.ergm.markov ), 
                  gden( padgbus.net ) ) , main='density') 
abline( v=gden( padgbus.net ),col='red' )# the observed density for reference


triads <- triad.census( g.ergm.markov , mode='graph' )
obsetriad <- triad.census(padgbus.net, mode='graph' )

hist(triads[,2] ,
     xlim= range( triads[,2] , obsetriad[2] ),
     main = 'single edge triads')
abline( v= obsetriad[2],col='red' )# the observed density for reference

hist(triads[,3] ,
     xlim= range( triads[,3] , obsetriad[3] ),
     main = 'open triads')
abline( v= obsetriad[3],col='red' )# the observed density for reference

hist(triads[,4] ,
     xlim= range( triads[,4] , obsetriad[4] ),
     main = 'closed triads')
abline( v= obsetriad[4],col='red' )# the observed density for reference

```

> The Markov model manages to reproduce all of these local, structural statistics

Let us look at a global property, like geodesic distances

```{r markovpadgetstructsrgraph, warning = FALSE}

row.index <- which(net.profiles$netnames=='PadgetB')# t

# because g.udegs is a list it is a little more complicated to use built in sna functions
distances <- matrix(0,10,100)
for (k in c(1:100))
{
  distances[,k] <- tabulate( geodist( g.ergm.markov[[k]] )$gdist[upper.tri(geodist( g.ergm.markov[[k]] )$gdist)], 
                             nbins=10 )  
}

matplot(c(1:(10)), distances ,
        type ='l',
        main='geodesic distribution' ,
        xlab='distance',
        ylab='frequency')
lines(c(1:(10)),
      tabulate( geodist( padgbus.net )$gdist[upper.tri(geodist( padgbus.net )$gdist)] , nbins=10 ),
      pch=24,bg="blue",lty=1,type='b',
      col='red',lwd=3 )


```

The Markov model does a pretty spectacular work of also capturing the
reach in the network

> The Markov model with 4 paramters is sufficient to explain density, local clustering, as well as connectivity

We can interpret this as the degree-based effects - the edges and
stars - cature mechanisms to do with popularity and activity; and, the
triangle parameter captures triadic closure - friends of my friends are
also my friends. There are many behvioural interpretations of these
mechanisms but at the end of the day they are sufficient for explaining
a lot of the network structure.

### Kapferer's tailors

Let us try an ERGM for Kapferer's tailors. This time, we use the
statistics edges and so-called alternating triangles. Alternating
triangles, or GWESP, counts the number of shared partners that two
directly tied individuals have. Each additional shared partner adds to
the statistic but with a geometrically decreasing weight.

```{r gwespERGMkapf, results='hide'}
g.ergm.gwesp <- simulate(KAPFTS1.net~edges+gwesp(decay = log(2) , fixed= TRUE),# one statistic
                    coef= c(-3.795, 1.075),# the two parameters
                    nsim=100,
                    control=control.simulate(MCMC.burnin=100000))# you need to bump up the default burnin, otherwise the networks are too similar to the starting point, the observed network

```

Inspect networks

```{r gwespkapf}
par( mfrow  = c(2,2) )
hist( gden( g.ergm.gwesp ) , # the density for each of the 100 random networks
      xlim=range( gden( g.ergm.gwesp ), 
                  gden( KAPFTS1.net ) ) , main='density') 
abline( v=gden( KAPFTS1.net ),col='red' )# the observed density for reference


triads <- triad.census( g.ergm.gwesp , mode='graph' )
obsetriad <- triad.census(KAPFTS1.net, mode='graph' )

hist(triads[,2] ,
     xlim= range( triads[,2] , obsetriad[2] ),
     main = 'single edge triads')
abline( v= obsetriad[2],col='red' )# the observed density for reference

hist(triads[,3] ,
     xlim= range( triads[,3] , obsetriad[3] ),
     main = 'open triads')
abline( v= obsetriad[3],col='red' )# the observed density for reference

hist(triads[,4] ,
     xlim= range( triads[,4] , obsetriad[4] ),
     main = 'closed triads')
abline( v= obsetriad[4],col='red' )# the observed density for reference

```

This two-paramter model seem to capture the local clustering well.

> An overall cost for ties (edges) and a tendency for people that have shared friends to also be friends explains most of the network strucure.

## Conclusions

Based on these experiments: 

* Assuming that ties form independely is
unrealistic
* Independent tie-formation may create short pathways
* Independence means that 
** The rich do not get richer (nodes do not know how many ties they already have)
** Closure does not happen (ties have 'no memory')
* The degree distribution only explain some network features
** Knowing what nodes are popular do not explain whom they form ties to

### Homework

Explore what other features the random graphs may or may not model

Pick a network, pick some ERGM terms by reference to `r 'ergm.terms'`
(use the help function `r '?ergm.terms'`).

Try to estimate a model and then simulate from it. How did I get my
parameter values? For Padgett is got the coefficients:

```{r ergmcall, eval=FALSE}
# don't run
ans <- ergm(padgbus.net~edges+kstar(2) +kstar(3) + triangles)#
summary(ans)# this produces an ANOVA table with coefficients
```

And for Kapferer:

```{r kapfergmcall, eval=FALSE}
# don't run
ans <- ergm(KAPFTS1.net~edges+gwesp(decay = log(2) , fixed= TRUE))#
summary(ans)# this produces an ANOVA table with coefficients
```

> Can you pick any statistic arbitrarily to include in the model?

There are at least three reasons the answer to this is no:

-   Higher order effects, like triangles, may be explained by the
    aggregation of lower-order statistics (such as two-stars)
-   Even if you want to model one specific statistic, it is not certain
    that you will be able to (see homework)
-   The Markov model and a model with GWESP can be represented in terms
    of principled dependence assumptions and other statistics do not
    have that interpretation

# Ego-nets

For standard surveys, respondents are sampled independently of each
other and we do not get any network data. The one piece of network
information we can get from a surevey is the number of contacts a
respondent has - this is the **degree** of the respondent.

> How much can we say about a network and a persons network position based only on independent reports?

Survey data may provide *egocentric*, as opposed to *socio-centric*,
network data. We call this **egonetworks** [@crossley2015social].

In the US General Social Survey, respondents have been asked to nominate
a range of different contacts. Here, we will

-   investigate the degree distributions of different relations,
-   examine to what extent we can explain the *degree centrality* using
    independent variables, and
-   examine the extent to which the *degree centrality* can explain
    other outcomes

The last sections (Fitting a distribution and Measurement error) provide
brief introductions to more advanced topics.

## Loading data

We are going to read in a stata file and for that we need the package

```{r foreignload, results='hide', warning=FALSE, message=FALSE}
library('foreign')
library('haven')# this package is needed to read Stata version 5-12 .dta file 
library('labelled')# this is needed to undo all the annoying things haven does
```

Download the General Social Survey data from the web first time around.
Going forward you may want to save it locally. You can download data
from <https://gssdataexplorer.norc.org/> and read in the downloaded
dataset (also the GSS for 1985) but here we will read the data straight
from the url:

```{r readgss, results='hide', warning=FALSE, message=FALSE}
#GSSdata <- read.dta( 'https://osf.io/hn5b9/download' , convert.factors=FALSE )
GSSdata <- as.data.frame( read_dta( 'https://osf.io/npjv3/download' ), stringsAsFactors=FALSE )
val_labels(GSSdata) <- NULL
```

For `r'read.dta'`, by setting `r 'convert.factors=FALSE'` we are using
numerical codes for categorical variables

## Data matrix

The dataset is a

```{r classgss}
class(GSSdata)
```

with dimensions

```{r dimgss}
dim(GSSdata)
```

constituting 2812 cases and 1181 variables. All variables are listed on
<http://www.thearda.com/Archive/Files/Codebooks/GSS2004_CB.asp>

#### Basic demographic variables

Basic variables include

| Number in list | variable name | name in GSSdata | description                     |
|----------------|---------------|-----------------|---------------------------------|
| (3)            | WRKSTAT       | wrkstat         | employment status               |
| (12)           | MARITAL       | marital         | marital status                  |
| (24)           | PAOCC80       | PAOCC80         | father's and mothers occupation |
| (27)           | MAOCC80       | MAOCC80         | mothers occupation              |
| (39)           | DEGREE        | degree          | education                       |
| (43)           | SEX           | sex             | sex of respondent               |
| (33)           | AGE           | age             | age of respondent               |

There are many measures of attritudes, for example political views (73
POLVIEW), towards government spending (74-101), race, racism, and
ethnicity, religion etc. In general there are also a lot of questions
about third parties, such as the household and household members. Many
questions consists of a long battery of targeted attitudes, such as
'confidence' in questions (163-176).

#### Psychological wellbeing

There are a variety of Psychological wellbeing measures

| Number in list | variable name | name in GSSdata | description                                              |
|------------------|-----------------|--------------------|-----------------|
| (157)          | HAPPY         | happy           | A happiness measure                                      |
| (160)          | LIFE          | life            | Excitement with Life                                     |
| (459)          | MNTLHLTH      | mntlhlth        | self-assesed (negative) mental health                    |
| (960)          | HLTH2         | HLTH2           | If underwent counseling for mental or emotional problems |

#### Scales, subscales, and single items

While there is no specific documentation or listing of scales used in
the survey, we recognise a number of questions as items from other
scales.

| Number in list | Scale                                                       | no. items | reference                        |
|------------------|----------------|---------------|-----------------------|
| (161 - 162)    | Trust in People Scale                                       | 3         | @yamagishi1986provision          |
| (462-468)      | Empathic Concern subscale of interpersonal reactivity index | 7         | @davis1980multidimensional       |
| (515-519)      | The Rosenberg Self-Esteem Scale                             | 5         | @rosenberg2015society            |
| (520-525)      | Subscale of Life orientation scale                          | 6         | @scheier1994distinguishing       |
| (524)          | OWNDOING, locus of control                                  | 1         | @rotter1966generalized           |
| (804-805)      | Trust in People Scale                                       | 2         | see e.g. @yamagishi1986provision |

It is worth looking through
<http://www.thearda.com/Archive/Files/Codebooks/GSS2004_CB.asp>
searching for terms of interest.

### Network summaries

The 2004 (and some earlier surveys) contain a number of network and
network-related variables.

| Variable name | Description                      | Note                                                             |
|:------------------------|----------------------------|-------------------|
| partners      | no. sex partners past year       | listed as PARTNERS in help file. Vars 931-938 further breakdowns |
| numwomen      | no. female sex partners          | listed as NUWOMEN in help file                                   |
| nummen        | no. male sex partners            | listed as NUMMEN in help file                                    |
| PARTNRS5      | no. sex partners in past 5 years | binned                                                           |
| numgiven      | no. close contacts               | truncated at 6+.                                                 |
| numcnt        | no. people keep in contact with  | this is other than family and work                               |
| 434-437       | mode of contact                  | keeping in contact with f-t-f, letters, email, etc               |

These are summaries of a (population) sexual network and a (population)
close contact network (a number of related network questions address
different aspect of social interaction).

Note: The variable HAPPY, the Trust (161 - 162) and EC (462-468) scales,
are not asked of people that provided 'numgiven'. Therefore it is
important to check cross tabulations before doing an analysis, for
example, tabulating happy against numgiven demonstrates that there are
no respondents with values on both.

```{r checkhappy, results='hide'}
table(GSSdata$happy,GSSdata$numgiven,useNA='always')
```

There were 6 versions of the survey administered and not all of them
included all questions, for example

```{r versionhappygiven,results='hide'}
table(GSSdata$version,GSSdata$happy, useNA='always')
table(GSSdata$version,GSSdata$numgiven, useNA='always')
```

## Number of sex partners

The networks of sexual contacts has received a lot of attention in
public health, see for example @morris2004network. The number of sexual
contacts also serves as a good illustration for modelling the degree
distribution of a network from survey data.

The number of sex partners is partially reported as binned and the
missing data code is 99

```{r sexmissing, results='hide'}
sexpart <- GSSdata$partners
sexpart[sexpart %in% c(9,989,99)] <- NA # missing data codes
sexpart[sexpart == 5 ] <- 7.5 # middle of range (4,10]
sexpart[sexpart == 6 ] <- 15 # middle of range (10,20]
sexpart[sexpart == 7 ] <- 60.5  # middle of range (20,100]
sexpart[sexpart == 8 ] <- 100 # 100+
```

The distribution is now

```{r sexparthist,results='hide'}
hist(sexpart)
```

And we can look at difference between male and female

```{r sexmale,results='hide'}

boxplot(sexpart~ GSSdata$sex)

```

However, testing the difference in number of sex partners between men an
women, there is a clear difference

```{r sexreg}
sexreg <- lm(sexpart~ GSSdata$sex==1)
summary(sexreg)
```

An equivalent way of testing this difference is by doing a t-test

```{r sexttest}
t.test( sexpart~ GSSdata$sex==1, var.equal = TRUE)
```

Note that the t-statistics are identical for the regression coefficient
and in the t-test.

> If these are the degree distributions in an undirected network, how come that there are such differences? [see also @spiegelhalter2015sex].

Because of this discrepancy and the binning we will focus solely on the
number of sexual partners of men in the sequel.

## Modelling partners

We are going to define $Y_i$ as the number of sex partners of a *male*
$i$.

> The variables $Y_i$ are the out-degrees of men in the network of sexual partners

We will model $Y_i$ using

-   Linear regression
-   Log-linear regression
-   Poisson regression
-   A power-law

Recode missing values for number of female sex partners and for the
variable pre-marital sex

```{r missnuwomen, results='hide'}
numwom <- GSSdata$nuwomen
numwom[numwom  > 993 ] <- NA # missing data codes
is.man <- GSSdata$sex==1
premarsx <- GSSdata$premarsx
premarsx[premarsx > 4] <- NA
```

### OLS

We assume
$$
Y_i = \beta_0 + \beta_1 x_{i} +\epsilon_i
$$
for $x_{i}$ being attitude to pre-marital sex and
$\epsilon_i \overset{i.i.d}{\thicksim}N(0,\sigma^2)$.

The OLS estimates are given by

```{r premarsex}
y <- numwom[ is.man ]# this for plotting reasons below
x <- premarsx[is.man ]# ditto
not.use <- is.na(y) | is.na(x)
y <- y[not.use==FALSE]
x <- x[not.use==FALSE]
nuwom.reg <- lm( y ~ x)
summary( nuwom.reg )
```

Check the residuals to see if the normality assumption appears satisfied

```{r premarsexres}
par(mfrow=c(1,2))
hist(nuwom.reg$residuals)
plot(x=predict(nuwom.reg), y= y,
     xlab='Predicted Values',
     ylab='Actual Values',
     main='Predicted vs. Actual Values')
abline(a=0, b=1)
```

> Residuals are clearly highly skewed and

### Log-normal

While the residuals are very skewed (and *not* normal), the logarithm of
the residuals might look different (we would have to do some
transformation as we cannot take the log of non-positive values).

Outcome variable needs to be strictly positive

```{r lognuwomen}
is.pos <- y>0
```

Letting $Z_i=\log(Y_i)$, we estimate
$$
Z_i = \alpha +\gamma X_{i} +\xi_i
$$

```{r lognuwomenreg}
log.nuwom.reg <- lm( log(y[is.pos]) ~ x[is.pos])
summary(log.nuwom.reg)
```

```{r premarsexlogres}
par(mfrow=c(1,2))
hist(log.nuwom.reg$residuals)
plot(x=predict(log.nuwom.reg), y= log(y[is.pos]),
     xlab='Predicted Values',
     ylab='Actual Values',
     main='Predicted vs. Actual Values')
abline(a=0, b=1)
```

## Poisson regression

The orginal degrees $Y_i$ are clearly **counts** and the Poisson
distribution may be a better model for counts
$$
E(Y_i \mid x_i ) = \lambda_i,\;\Pr(Y_i=h\mid x_i)=e^{-\lambda_i}\frac{\lambda_i^h}{h!},{\text{ and }}\log(\lambda_i)=\eta_i=\beta_0+\beta_1x_i
$$

We estimate $\beta_0$ and $\beta_1$ using `r'glm'`

```{r poreggr}
reg.po <- glm( y ~ x, family = poisson(link='log') )
summary( reg.po )
```

## Power-law

In addition to standard statistical distributions, a number of models
for networks have been proposed that give rise to distributions for the
degree distribution. One such model, is the mathematical mechanism
preferential attachment which is claimed to give rise to a degree
distribution that is distributed as a *power-law*
[@barabasi1999emergence].

@liljeros2001web make very strong claims about the distribution of sex
partners - take-home point is 'is degree the only determinant of
connectivity and spread?'

$$
\Pr(Y=k) \propto k^{-\alpha} {\text{ , for }} k > k_c
$$

Drawing on epidemiology that suggests that any disease spread on the
network becomes epidemic if $\alpha \leq 3$.

In their paper, @barabasi1999emergence, they calim extensively that that
the power-law form proves that the only mechanims that is at play in
networks is preferential attachement (wrong because it isn't and there
is not a one-to-one between the power-law and preferential
attachement)[@koskinen2008degree]

### Modelling the degree distribution

The package `` r`'poweRlaw' `` can be used to fit different functional
forms to the degree distribution. To use it first time, install it.

```{r installplaw, eval=FALSE, results='hide', warning=FALSE, message=FALSE}
install.packages('poweRlaw')
```

Load the package

```{r loadpoweRl, results='hide', warning=FALSE, message=FALSE}
library('poweRlaw')
```

The power-law does not straightforwardly allow explantory variables so
let us simply re-estimate the **log-normal** and **Poissson** without
predictors for comparison. Note, the power-law **cannot handle zeros**,
so now we estimate the degree distributions **conditional** on $Y_i>0$

```{r fitpowerlaw, results='hide',message=FALSE}
m_pl = displ$new( y[is.pos] ) # fit powerlaw
est = estimate_xmin(m_pl)# get estimates
m_pl$setXmin(est)# set the minimum vaulue
m_ln = dislnorm$new( y[is.pos] ) # fit lognormal dist 
est_ln = estimate_xmin(m_ln) # set minimum value
m_ln$setXmin(est_ln)
m_pois = dispois$new( y[is.pos] ) # poisson 
m_pois$setXmin(1)
est_pois = estimate_pars(m_pois)
m_pois$setPars(est_pois)
```

Note that `r 'm_pl$setXmin'` estimates the value of degree $k_c$ from
which the power-law holds.

The program allows us to plot the fitted distrubution, more specifically
the 'survival functions' $\Pr(Y > k)$ on a log-log scale.

```{r plotpowerlaw}
plot(m_pl)# plots the raw data
lines(m_pl, col='red') # plots the power law
lines(m_ln, col='blue') # plots the lognormal
lines(m_pois, col='green') # plots the poisson
```

> What distribution seems to best fit the number of female sex partners?
> Two things to note: Poisson has been forced to ignore 0 and, secondly,
> the power-law has ignored all the information below $k_c$

## Core discussion partners

@fischer20092004 and @bailey1999interpretation and others [e.g.
@marsden2003interviewer; @putnam2001social; @mcpherson2006social] have
used various waves of the GSS to investigate 'the size' of peoples'
networks. The size of peoples' networks has been the target of intense
speculation since Robin @dunbar1992neocortex proposed that there was an
evolutionary limit on the number of close contacts that humans (or
primates?) can retain in their brain.

The key outcome varaible in these studies has been the number of named
core discussion partners

```{r tablecont}
table( GSSdata$numgiven , useNA= 'always')
```

The value 9 is a missing data code and recall that the number of named
contacts is truncated at 6 - a respondent that nominated 6 or more has
`r 'numgiven'` coded as 6 but was only allowed to provide 5 names.

As for a sociocentric network, we can plot the (truncated) degree
distribution.

```{r degreedistcont}
GSSdata$numgiven[GSSdata$numgiven>6] <- NA
plot( table( GSSdata$numgiven ) )
```

> Without knowing the structure of the population network we can still estimate the degree distribution

### Explaining the degree distribution

As we did for sexual partners, we may try to explain the number of close
contacts by using statistical tests. While both sexual partners and
close contacts both have skewed degree distributions, there is one
notably difference in GSS, namely the range of values. Close contacts
range from zero to 6 and sexual partners range from 0 to 900 or so. This
limits the use of

-   approximating close contacts using a continuous distribution, and
-   removing isolates (degree 0) removes an informative portion of data.

### Regressions

Close contacts might act as social support and alleviate mental health
issues [@bryant2017mental]. Can we test whether better mental health is
associated with having more social support?

Create a binary variable indicating whether a respondent has experienced
1 or more days of mental ill health in the past year.

```{r mnthlth, results='hide'}
GSSdata$mntlhlth[ GSSdata$mntlhlth > 30 ] <- NA # set as missing
illhealth <- GSSdata$mntlhlth>0 # this will indicate one day or more
```

### OLS

Again, assuming random normal errors

```{r olsreggrconts}
reg.close.lm <- lm(GSSdata$numgiven ~ illhealth )
summary( reg.close.lm )
```

### Poisson

Using the log-link function

```{r poreggrconts}
reg.close.po <- glm(GSSdata$numgiven ~ illhealth, family = poisson(link='log') )
summary( reg.close.po )
```

```{r}
par(mfrow=c(1,2))
plot(x=predict(reg.close.lm), y= GSSdata$numgiven[!is.na(GSSdata$numgiven) & !is.na(illhealth)],
     xlab='Predicted Values',
     ylab='Actual Values',
     main='OLS Predicted vs. Actual Values')
abline(a=0, b=1)

plot(x=reg.close.po$fitted.values, y=  GSSdata$numgiven[!is.na(GSSdata$numgiven) & !is.na(illhealth)],
     xlab='Predicted Values',
     ylab='Actual Values',
     main='Po Predicted vs. Actual Values')
abline(a=0, b=1)
```

> The predicted values are just $\hat{\beta}_0$ and $\hat{\beta}_0+\hat{\beta}_1$, and $e^{\hat{\beta}_0}$ and $e^{\hat{\beta}_0+\hat{\beta}_1}$, for the OLS and Poisson, respectively. Since the Poisson does not predict negative values, the fit is better

```{r}
par(mfrow=c(2,1))
plot(table(GSSdata$numgiven[illhealth==0]),ylab='',main='Degree distribution healthy')
abline(v=min(reg.close.po$fitted.values),col='red')
lines(mean(GSSdata$numgiven[illhealth==0],na.rm=TRUE),0,pch=15,type='p')
plot(table(GSSdata$numgiven[illhealth==1]),ylab='',main='Degree distribution long term un-healthy')
abline(v=max(reg.close.po$fitted.values),col='red')
lines(mean(GSSdata$numgiven[illhealth==1],na.rm=TRUE),0,pch=15,type='p')
```

##### Other tests

All the usual statistical tests are avaialble in R. For example ANOVA
<https://www.statmethods.net/stats/anova.html>.

## Ego-alter ties

In addition to naming alters, each ego has also provided some basic
information about each of their named alters

![Ego-alter
tie](https://raw.githubusercontent.com/johankoskinen/CHDH-SNA/main/images/egoalter.png)

Here we will

-   transform data from the *wide format* to the so-called *long
    format*, so that
-   each Ego-Alter pair is represented by a separate case or data point

We will use the GSS 2004

## Properties of 'alters'

The number of named alters per person is given by the variable
`numgiven`, which is the number of close contacts, truncated at 6+. For
each of these named contacts, ego is providing descriptions in the form
of

| Variable name | Description                             | Note                               |
|-------------------------------|------------------------|------------------|
| SEX1          | sex of first named alter                | this is NA if less than one named  |
| $\vdots$      |                                         |                                    |
| SEX5          | sex of fifth named alter                | this is NA if less than five named |
| RACE1         | sex of first named alter                | this is NA if less than one named  |
| $\vdots$      |                                         |                                    |
| RACE5         | sex of fifth named alter                | this is NA if less than five named |
| TALKTO1       | Talks to first person important matters | this is NA if less than one named  |
| $\vdots$      |                                         |                                    |
| TALKTO5       | Talks to first person important matters | this is NA if less than five named |
| EDUC1         | education of first named alter          | this is NA if less than one named  |
| $\vdots$      |                                         |                                    |
| EDUC55        | education of fifth named alter          | this is NA if less than five named |

There are also a hoast of questions about commong group membership (e.g.
GRPBOTH1 through GRPBOTH5) and religion, etc.

### Examples of named alters

Attributes for named alters are reported in the *wide format*: for each
respondent values are reported in ascending order by alter id, variable
by variable.

#### Case no nominated alters

Respondent has not named any alters, so there are no alter properties
reported

```{r altalt1, results='hide'}
GSSdata[1,261:270]
```

#### Case one nominated alter

Respondent has named exactly one alters, so there are only varible
values for 'SEX1', 'RACE1', etc

```{r altalt11, results='hide'}
GSSdata[11,261:270]
```

```{r altalt11sex, results='hide'}
GSSdata$SEX1[11]
```

#### Case three nominated altera

Respondent has named exactly three alters, so there are three reports
for each of the variables 'SEX1', 'RACE1', etc.

```{r altalt13, results='hide'}
GSSdata[13,261:270]
```

These 3 alters are all female

```{r altalt13sex, results='hide'}
GSSdata[13,271:275]
```

#### Case four nominated altera

Respondent has named exactly four alters, so there are four alter values
to report on each variable

```{r altalt16sex, results='hide'}
GSSdata[16,271:275]
```

```{r altalt16fam, results='hide'}
GSSdata[16,301:305]
```

```{r altalt16cowork, results='hide'}
GSSdata[16,306:310]
```

## Homophily

For egos 13, 16 and 42, the age of their alters is given in

```{r agealt, results='hide'}
GSSdata[c(13,16,42),c(346:350)]
```

we can thus get the average age in each egonet using

```{r aveagealt, results='hide'}
GSSdata$numgiven[GSSdata$numgiven>6] <- NA
ave.age <- rowSums(GSSdata[,c(346:350)], na.rm=TRUE)/GSSdata$numgiven 
```

Compare the age of ego with

```{r plaveagealt, results='hide'}
plot( GSSdata$age[GSSdata$numgiven %in% c(1:5)] ,  ave.age[GSSdata$numgiven %in% c(1:5)])
```

**Homophily** [@mcpherson2001birds] is the process of people choosing
contacts that are like themselves. If there is homophily on age, we wold
expect the averge age of alter to be predicted to the age of ego

```{r regaveagealt}
reg.age <- lm( ave.age[GSSdata$numgiven %in% c(1:5)] ~ GSSdata$age[GSSdata$numgiven %in% c(1:5)] )
summary(reg.age)
```

We can also investigate if there is homophily on race. Race is coded
differently for respondents and alters so we need to recode

```{r egowhite, results='hide'}
ego.white <- GSSdata$race==1
alter.white <- GSSdata[,c(276,  277 , 278 , 279 , 280)]==4
prop.alter.white <- rowSums(alter.white,  na.rm=TRUE)/GSSdata$numgiven
boxplot(prop.alter.white ~ ego.white )
```

There is clear evidence of homophily on race.

#### Diversity

In terms of access to resources, it may be important to have access to
different types of resources. For each ego we can tabulate the number of
alters in each category

```{r atertabedu}
alter.education <- cbind(
  rowSums(GSSdata[,c(341:345)]==0, na.rm=TRUE),
  rowSums(GSSdata[,c(341:345)]==1, na.rm=TRUE),
  rowSums(GSSdata[,c(341:345)]==2, na.rm=TRUE),
  rowSums(GSSdata[,c(341:345)]==3, na.rm=TRUE),
  rowSums(GSSdata[,c(341:345)]==4, na.rm=TRUE),
  rowSums(GSSdata[,c(341:345)]==5, na.rm=TRUE),
  rowSums(GSSdata[,c(341:345)]==6, na.rm=TRUE),
  rowSums(GSSdata[,c(341:345)]==7, na.rm=TRUE),
  rowSums(GSSdata[,c(341:345)]==8, na.rm=TRUE))
```

Based on this we can calculate the proportions $p_j$ of alters in
category $j$.

```{r proptalteredu}
alter.education <- alter.education / rowSums(alter.education)# convert to proportions
```

Simpsons dissimilarity index
$$
H = \sum_{j=1}^R p_j
$$
[@simpson1949measurement; see discussion of simmilarity indices for
networks in @stys2020brokering]

```{r dissindex}
diss.index <- rowSums(alter.education^2,na.rm=TRUE)
```

## Converting to long format

**Long format** means that we code each ego-alter variable as a separate
observation. Manually we can create a new data frame where alter-id is
implied by '\_1', '\_2', etc.

```{r lognform}
         d1 <- data.frame(id = GSSdata$id,
         egosex = GSSdata$sex,
         egoage = GSSdata$age,
         sex_1 = GSSdata$SEX1,
         sex_2 = GSSdata$SEX2,
         sex_3 = GSSdata$SEX3,
         sex_4 = GSSdata$SEX4,
         sex_5 = GSSdata$SEX5,
         talkto_1 = GSSdata$TALKTO1,
         talkto_2 = GSSdata$TALKTO2,
         talkto_3 = GSSdata$TALKTO3,
         talkto_4 = GSSdata$TALKTO4,
         talkto_5 = GSSdata$TALKTO5,
         age_1 = GSSdata$AGE1,
         age_2 = GSSdata$AGE2,
         age_3 = GSSdata$AGE3,
         age_4 = GSSdata$AGE4,
         age_5 = GSSdata$AGE5,
         cowork_1 = GSSdata$COWORK1,
         cowork_2 = GSSdata$COWORK2,
         cowork_3 = GSSdata$COWORK3,
         cowork_4 = GSSdata$COWORK4,
         cowork_5 = GSSdata$COWORK5
         )
```

This is still wide-format, but it is a data frame that only has the
subset of variables that we chose. To convert it to 'long format', we
use the `r 'reshape'` function

```{r longformat}
egoalter <- reshape(d1, # our wide format data frame
                    dir = "long", # indicate that we want to transfrom into 'long' format
                    varying = 4:23, # variables 4 through 23 vary across egos
                    sep = "_") # this is the charachter used to indicate what ego aler belongs to

# This reshape function does not work on 'haven'-formatted data with values
```

Have a look at the new, long-format data frame

```{r inspectlongformat}
head(egoalter)
```

Recode `r 'talkto'` to a binary variable capturing 'almost daily'

```{r recondetalkto}
egoalter$talkto[egoalter$talkto > 4] <- NA
egoalter$talkto[egoalter$talkto>1] <- 0
```

### Multilevel model of ego-alter ties

Consider now modelling talk to almost daily as a binary variable
$Y_{ij}$, where $i$ is ego and $j$ is alter 
$$
\mathrm{logit} (\Pr(Y_{ij}=1|z_i,x_j))=\alpha+\beta z_i +\gamma x_i + \nu_i,
$$
where $\nu_i \thicksim N(0,\tau^2)$ is a random effect for ego $i$
that is shared across all alters of $i$.

To do this using a logistic link function we need the package `lme4`

```{r loadlme42, results='hide', warning=FALSE, message=FALSE}
library(lme4)
```

```{r mixedlogit, warning=FALSE}
 reg.talkto <- glmer(talkto ~ egoage+egosex+sex+age+cowork+(1| id), # regression formula
                     data= egoalter, # name of the data frame that holds the variables
                     family= binomial)
summary(  reg.talkto )
```


# References
